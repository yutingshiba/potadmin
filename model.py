# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cB0E5S3RjG58jQb-zFmrhGwLp3LJkvlN
"""

import numpy as np
import tensorflow as tf
import tensorflow.keras as kr
import tensorflow.keras.layers as ly
#import tensorflowjs as tfjs
import data_processor

#global parameters
epochs=5
train_data_size=65536#deprecated
test_data_size=4096#deprecated
sentence_length=64
batch_size=512
embedding_size=300
embedding_maxindex=1000
rnn_size=2
rnn_length=64
dense_size=4
dense_layer=[32,8,4,1]
learning_rate=0.0001
train_path='./_data/EI_train.csv'
test_path='./_data/EI_test.csv'
valid_path='./_data/EI_valid.csv'
#random data
train_data=np.random.random_integers(embedding_maxindex,size=(train_data_size,sentence_length,embedding_size))
train_label=np.random.random_integers(2,size=(train_data_size))
test_data=np.random.random_integers(embedding_maxindex,size=(test_data_size,sentence_length,embedding_size))
test_label=np.random.random_integers(2,size=(test_data_size))

#true data
word_vec=data_processor.load_emb_file('wiki.en.vec')
count0=0
count1=0

#define metric
class Metric(Callback):
  def on_train_begin(self, logs={}):
    self.val_f1s = []
    self.val_recalls = []
    self.val_precisions = []

  def on_epoch_end(self, epoch, logs={}):
    val_predict=(np.asarray(self.model.predict(self.model.validation_data[0]))).round()
    val_targ = self.model.validation_data[1]
    _val_f1 = f1_score(val_targ, val_predict)
    _val_recall = recall_score(val_targ, val_predict)
    _val_precision = precision_score(va_targ, val)
    self.val_f1s.append(_val_f1)
    self.val_recalls.append(_val_recall)
    self.val_precisions.append(_val_precision)
    print('-val_f1: %.4f --val_precision: %.4f --val_recall: %.4f'%(_val_f1, _val_precision, _val_recall))
    return
metric = Metric()

#model building
model=kr.Sequential()
model.add(ly.Bidirectional(ly.LSTM(rnn_length,return_sequences=True),
                        merge_mode='concat',
                        input_shape=(sentence_length,embedding_size)))
for i in range(0,rnn_size-1):
  model.add(ly.Bidirectional(ly.LSTM(rnn_length,return_sequences=True)))
model.add(ly.Flatten())
for i in range(0,dense_size):
  model.add(ly.Dense(dense_layer[i],
                     activation='relu',
                     bias_regularizer=kr.regularizers.l2(0.01),
                    kernel_initializer='orthogonal'))
model.compile(optimizer=kr.optimizers.Adam(learning_rate),
             loss='binary_crossentropy',
             metrics=['binary_accuracy',metric]
             )
callbacks = [
  tf.keras.callbacks.EarlyStopping(patience=2, monitor='loss'),
  tf.keras.callbacks.TensorBoard(log_dir='./logs')
]
print(model.summary())
model.fit_generator(generator=data_processor.generate_arrays_from_file(path=train_path,
                                                                       batch_size=batch_size,
                                                                       word_vec=word_vec),
            steps_per_epoch=data_processor.get_size(train_path)//batch_size,
            epochs=epochs,
            validation_data=data_processor.generate_arrays_from_testfile(path=test_path,
                                                                         batch_size=batch_size,
                                                                         word_vec=word_vec),
            validation_steps=data_processor.get_size(test_path)//batch_size
            callbacks=callbacks)
model.save('model.h5')
#tfjs.converters.save_keras_model(model, 'model.json')#save tf.js model,if need
print('model saved successfully')
#print('test begin')
#model.evaluate_generator(generator=data_processor.generate_arrays_from_file(path=test_path,batch_size=batch_size,word_vec=word_vec),
#        steps=len(test_data)//batch_size)
#print('test end')


