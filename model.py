# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cB0E5S3RjG58jQb-zFmrhGwLp3LJkvlN
"""

import numpy as np
import tensorflow as tf
import tensorflow.keras as kr
import tensorflow.keras.layers as ly
#global parameters
epochs=10
train_data_size=4096#the number of post
sentence_length=64
test_data_size=512
batch_size=512
embedding_size=64
embedding_maxindex=1000
rnn_size=2
rnn_length=64
dense_size=4
dense_layer=[32,8,4,1]
learning_rate=0.0001

#random data
data=np.random.random_integers(embedding_maxindex,size=(train_data_size,sentence_length,embedding_size))
label=np.random.random_integers(2,size=(train_data_size))
print(np.array(data).shape)

#model building
model=kr.Sequential()
model.add(ly.Bidirectional(ly.LSTM(rnn_length,return_sequences=True),
                        merge_mode='concat',
                        input_shape=(sentence_length,embedding_size)))
for i in range(0,rnn_size-1):
  model.add(ly.Bidirectional(ly.LSTM(rnn_length,return_sequences=True)))
model.add(ly.Flatten())
for i in range(0,dense_size):
  model.add(ly.Dense(dense_layer[i],
                     activation='relu',
                     bias_regularizer=tf.keras.regularizers.l2(0.01),
                    kernel_initializer='orthogonal'))
model.compile(optimizer=tf.train.AdamOptimizer(learning_rate),
             loss='binary_crossentropy',
             metrics=['binary_accuracy']
             )
print(model.summary())
model.fit(data,label,epochs=epochs,batch_size=batch_size)



