# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cB0E5S3RjG58jQb-zFmrhGwLp3LJkvlN
"""

import numpy as np
import tensorflow as tf
import tensorflow.keras as kr
import tensorflow.keras.layers as ly
#import tensorflowjs as tfjs
import data_processor

#global parameters
epochs=5
train_data_size=65536#deprecated
test_data_size=4096#deprecated
sentence_length=64
batch_size=512
embedding_size=300
embedding_maxindex=1000
rnn_size=2
rnn_length=64
dense_size=4
dense_layer=[32,8,4,1]
learning_rate=0.0001
train_path='./_data/EI_train.csv'
test_path='./_data/EI_test.csv'
valid_path='./_data/EI_valid.csv'
#random data
train_data=np.random.random_integers(embedding_maxindex,size=(train_data_size,sentence_length,embedding_size))
train_label=np.random.random_integers(2,size=(train_data_size))
test_data=np.random.random_integers(embedding_maxindex,size=(test_data_size,sentence_length,embedding_size))
test_label=np.random.random_integers(2,size=(test_data_size))

#true data
word_vec=data_processor.load_emb_file('wiki.en.vec')
count0=0
count1=0

#define metric
def tp(y_true,y_pred):
    return kr.backend.sum(y_true*kr.backend.round(y_pred)/y_pred.shape[0])
def tn(y_true, y_pred):
    return kr.backend.sum((1-y_true) * (1-kr.backend.round(y_pred))/y_pred.shape[0])
def fp(y_true, y_pred):
    return kr.backend.sum(y_true * (1-kr.backend.round(y_pred))/y_pred.shape[0])
def fn(y_true, y_pred):
    return kr.backend.sum((1-y_true)*kr.backend.round(y_pred)/y_pred.shape[0])
def precision(y_true,y_pred):
    return tp(y_true,y_pred)/(tp(y_true,y_pred)+fp(y_true,y_pred))
def recall(y_true,y_pred):
    return tp(y_true,y_pred)/(tp(y_true,y_pred)+fn(y_true,y_pred))
def f1(y_true,y_pred):
    return 2./(1./recall(y_true,y_pred)+1./precision(y_true,y_pred))

#model building
model=kr.Sequential()
model.add(ly.Bidirectional(ly.LSTM(rnn_length,return_sequences=True),
                        merge_mode='concat',
                        input_shape=(sentence_length,embedding_size)))
for i in range(0,rnn_size-1):
  model.add(ly.Bidirectional(ly.LSTM(rnn_length,return_sequences=True)))
model.add(ly.Flatten())
for i in range(0,dense_size):
  model.add(ly.Dense(dense_layer[i],
                    activation='relu',
                    bias_regularizer=kr.regularizers.l2(0.01),
                    kernel_initializer='orthogonal'))
model.compile(optimizer=kr.optimizers.Adam(learning_rate),
             loss='binary_crossentropy',
             metrics=['binary_accuracy',f1,tp,tn,fp,fn]
             )
callbacks = [
  tf.keras.callbacks.EarlyStopping(patience=2, monitor='loss'),
  tf.keras.callbacks.TensorBoard(log_dir='./logs')
]
print(model.summary())
model.fit_generator(generator=data_processor.generate_arrays_from_file(path=train_path,
                                                                       batch_size=batch_size,
                                                                       word_vec=word_vec),
            steps_per_epoch=data_processor.get_size(train_path)//batch_size,
            epochs=epochs,
            validation_data=data_processor.generate_arrays_from_testfile(path=test_path,
                                                                         batch_size=batch_size,
                                                                         word_vec=word_vec),
            validation_steps=data_processor.get_size(test_path)//batch_size,
            callbacks=callbacks)
model.save('model.h5')
#tfjs.converters.save_keras_model(model, 'model.json')#save tf.js model,if need
print('model saved successfully')
#print('test begin')
#model.evaluate_generator(generator=data_processor.generate_arrays_from_file(path=test_path,batch_size=batch_size,word_vec=word_vec),
#        steps=len(test_data)//batch_size)
#print('test end')


